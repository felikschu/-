## 2024.12.15
昨天学习了感知机算法以及部分的支持向量机算法。说实话学习过程中有一种深刻的感触：数学没白学。不仅没白学，而且还远远没学够。

例如最简单的感知机模型，就是解决一个线性可分数据集的二分类问题，而且分类结果也不一定是唯一的。这里面的学习算法就是随机梯度下降，是本学期运筹学课上的内容。此外，在上学期的数值分析中也学习过。
很多时候数学系的学生会抱怨说学了一大堆没用的知识，但是现在接触了大创和一些统计学习内容，才发现这些知识确实都是必要的，如果没有事先学过这些内容的话就会导致学习曲线异常陡峭，初学者就没法把握算法思想的脉络，
而是深陷在数学概念与数学推导中。

再比如说支持向量机，这里面对数学的要求就更高了。这里面使用了诸多凸优化工具，比如，在**线性支持向量机与硬间隔最大化**上，用到了拉格朗日对偶性，KTT条件等，这些都是运筹学课程上的内容。首先，它把一个原始最优化问题
表示为了广义拉格朗日函数的极小极大问题，然后再考虑这个极小极大问题的对偶问题，利用解对偶问题来替代原始问题。再比如，在**线性支持向量机与软间隔最大化**上，由于线性不可分，因此要引入一个松弛变量,并在
目标函数中增加一个关于这个松弛变量的项作为补偿。以及，在**非线性支持向量机与核函数**中，运用了非常多的数学基础知识，比如构造希尔伯特空间等。

之前的slam大创项目亦是如此，其中运用了李代数的知识来刻画物体在空间中的运动，以及运用了诸多运筹优化、数值分析的内容来处理数据。

下学期我将学习《实变函数与泛函分析》，其实有理由相信，这门课相当重要，而且可以说是后续诸多统计学、机器学习的基础。例如应用随机过程，这门课里面有非常多的定义与概念甚至题目，都是用测度论的语言来刻画的，如果没有学习过实变函数就会相当吃力。

*******
今天晚上尝试了一下支持向量机的代码实现，做了一个kaggle上的泰坦尼克号的生还预测，结果正确率只有百分之78，得分0.765。其实在写代码的时候能感觉到，这和我学的东西完全就是两个东西，因为课本讲的是原理，而代码实现得完全就是调一个包，甚至都不需要知道具体算法。那么时间都花在哪里了呢？主要是数据清洗以及环境配置上。感觉学习书本上的原理做的是数学证明，但是真正跑代码的时候却完全不需要用到，更多的任务其实是在数据的清洗上，没错，就是在这个地方大做文章。当然，也有可能这是模型本身的缺陷，因为其实有可能这个数据集本身是线性不可分的，需要用一些别的方法来做预测。对于美赛来说，或许数学部分完全是不需要的，只需要会写代码会调包即可。
